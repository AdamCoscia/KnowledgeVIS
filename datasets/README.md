# KnowledgeVIS datasets

üóÇÔ∏è Subsets of the data that we used to create the prompts used in our case studies.

üìÑ For more details, please read our paper: <https://adamcoscia.com/papers/knowledgevis/>

## PubMedQA: A Dataset for Biomedical Research Question Answering (2019)

*The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts.*

> Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. [PubMedQA: A Dataset for Biomedical Research Question Answering](https://aclanthology.org/D19-1259). In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567‚Äì2577, Hong Kong, China. Association for Computational Linguistics.

- For more information, see: <https://github.com/pubmedqa/pubmedqa>

## BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation (2021)

*Bias in Open-ended Language Generation Dataset (BOLD) is a dataset to evaluate fairness in open-ended language generation in English language. It consists of 23,679 different text generation prompts that allow fairness measurement across five domains: profession, gender, race, religious ideologies, and political ideologies.*

> Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21). Association for Computing Machinery, New York, NY, USA, 862‚Äì872. <https://doi.org/10.1145/3442188.3445924>

- For more information, see: <https://github.com/amazon-science/bold>

## HONEST: Measuring Hurtful Sentence Completion in Language Models

*Large language models (LLMs) have revolutionized the field of NLP. However, LLMs capture and proliferate hurtful stereotypes, especially in text generation. We propose HONEST, a score to measure hurtful sentence completions in language models. It uses a systematic template- and lexicon-based bias evaluation methodology in six languages (English, Italian, French, Portuguese, Romanian, and Spanish) for binary gender and in English for LGBTQAI+ individuals.*

> Nozza D., Bianchi F., and Hovy D. "HONEST: Measuring hurtful sentence completion in language models." The 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2021. <https://aclanthology.org/2021.naacl-main.191>

> Nozza D., Bianchi F., Lauscher L., and Hovy D. "Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals." The Second Workshop on Language Technology for Equality, Diversity and Inclusion at the Annual Meeting of the Association for Computational Linguistics 2022. <https://aclanthology.org/2022.ltedi-1.4/>

- For more information, see: <https://github.com/MilaNLProc/honest>

## LAMA: LAnguage Model Analysis

*LAMA is a probe for analyzing the factual and commonsense knowledge contained in pretrained language models.*

> Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. [Language Models as Knowledge Bases?](https://aclanthology.org/D19-1250). In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463‚Äì2473, Hong Kong, China. Association for Computational Linguistics.

> Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt√§schel, Yuxiang Wu, Alexander H. Miller, & Sebastian Riedel (2020). How Context Affects Language Models' Factual Predictions. In *Automated Knowledge Base Construction*.

- For more information, see: <https://github.com/facebookresearch/LAMA>
